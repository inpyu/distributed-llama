# Distributed LLM Network Optimization: O(nÂ²) â†’ O(n)

## ğŸ¯ ë¬¸ì œ ì •ì˜

### ì´ˆê¸° ìƒí™©
**distributed-llamaì˜ ë„¤íŠ¸ì›Œí¬ ë³‘ëª©:**
- **ì•Œê³ ë¦¬ì¦˜**: All-to-All í†µì‹  (O(nÂ²))
- **ë¬¸ì œì **: ê° ë…¸ë“œê°€ ëª¨ë“  ë‹¤ë¥¸ ë…¸ë“œì™€ í†µì‹ 
- **ë³µì¡ë„**: nê°œ ë…¸ë“œ â†’ nÃ—(n-1) = O(nÂ²) í†µì‹  ì—°ì‚°

### ì„±ëŠ¥ ì¸¡ì • (8 ë…¸ë“œ)
```
SYNC_NODE_SLICES: 245 ops, 16.27 MB, 434.66 ms avg (ëŒ€í˜• ëª¨ë¸)
SYNC_NODE_SLICES: 245 ops, 8.11 MB, 5.76 ms avg (ì¼ë°˜ ëª¨ë¸)
```

**ëª©í‘œ**: O(nÂ²) â†’ **O(log n)** ë˜ëŠ” **O(n)**ìœ¼ë¡œ ê°œì„ 

---

## ğŸ”¬ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ì˜ í•´ê²° ë°©ë²•

### 1. TensorRT-LLM

**í•µì‹¬ ê¸°ìˆ : NCCL (NVIDIA Collective Communications Library)**

```python
# TensorRT-LLM ì½”ë“œ
import torch.distributed as dist
dist.all_gather_into_tensor(output, input, group=device_group)
```

**NCCLì˜ íŠ¹ì§•:**
- âœ… **GPU ê¸°ë°˜**: CUDAë¥¼ í™œìš©í•œ ê³ ì† í†µì‹ 
- âœ… **Ring Algorithm**: O(n) ë³µì¡ë„
- âœ… **Pipelining**: ë°ì´í„°ë¥¼ chunkë¡œ ë‚˜ëˆ  ë™ì‹œ ì „ì†¡
- âœ… **ë¹„ë™ê¸° ì²˜ë¦¬**: CUDA streams í™œìš©
- âœ… **ê²€ì¦ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬**: NVIDIAê°€ ìµœì í™”

**ë‚´ë¶€ êµ¬í˜„:**
```
Ring All-Gather with Pipelining:
Step 0: Chunk 0 ì „ì†¡ ì‹œì‘ + Chunk 1 ì¤€ë¹„
Step 1: Chunk 0 ì™„ë£Œ ë™ì‹œì— Chunk 1 ì „ì†¡
â†’ Latency hiding through pipelining
```

**ì œì•½ì‚¬í•­:**
- âŒ GPU ì „ìš© (CUDA required)
- âŒ CPUì—ì„œ ì‚¬ìš© ë¶ˆê°€
- âŒ distributed-llamaëŠ” CPU ê¸°ë°˜

---

### 2. vLLM

**í•µì‹¬ ê¸°ìˆ : Shared Memory + Process Group**

#### GPU ëª¨ë“œ
```python
# NCCL ì‚¬ìš© (TensorRT-LLMê³¼ ë™ì¼)
from vllm.distributed import CudaCommunicator
comm.all_gather(tensor)  # NCCL backend
```

#### **CPU ëª¨ë“œ (ìš°ë¦¬ê°€ ì°¸ê³ í•  ë¶€ë¶„!)**

```cpp
// vLLM CPU all-gather êµ¬í˜„ (csrc/cpu/shm.cpp)
void shm_all_gather(int64_t handle, const torch::Tensor& data, torch::Tensor& output) {
    auto ctx = SHMManager::get_singleton_instance(handle)->get_shm_ctx();
    
    // 1. ê° í”„ë¡œì„¸ìŠ¤ê°€ shared memoryì— ìê¸° ë°ì´í„° write
    scalar_t* thread_shm_ptr = thread_ctx->get_thread_shm_ptr<scalar_t>(rank);
    shm_cc_ops::memcpy(thread_shm_ptr, data, data_size);
    
    // 2. ë™ê¸°í™” barrier - ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ write ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    thread_ctx->wait_for_all(ThreadSHMContext::check_no_buffer_conflict);
    thread_ctx->commit_ready_stamp();
    
    // 3. ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ shared memoryì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ë°ì´í„° read
    for (int i = 0; i < world_size; ++i) {
        scalar_t* src_ptr = thread_ctx->get_thread_shm_ptr<scalar_t>(i);
        thread_ctx->wait_for_one(i, ThreadSHMContext::check_stamp_ready);
        shm_cc_ops::memcpy(output[i], src_ptr, data_size);
    }
}
```

**vLLM CPU ë°©ì‹ì˜ í•µì‹¬:**

1. **Shared Memory IPC**
   - í”„ë¡œì„¸ìŠ¤ ê°„ ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš©
   - Network I/O ì—†ìŒ (ë©”ëª¨ë¦¬ copyë§Œ)
   - ê°™ì€ ë¨¸ì‹ ì—ì„œë§Œ ì‘ë™

2. **Synchronization Primitives**
   - `wait_for_all()`: ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸°
   - `commit_ready_stamp()`: ì™„ë£Œ ì‹ í˜¸
   - `wait_for_one()`: íŠ¹ì • í”„ë¡œì„¸ìŠ¤ ëŒ€ê¸°

3. **ì¥ì **
   - âœ… ë§¤ìš° ë¹ ë¦„ (ë©”ëª¨ë¦¬ ì†ë„)
   - âœ… Thread-safe
   - âœ… Zero-copy (ê°™ì€ ë©”ëª¨ë¦¬ ê³µê°„)

4. **ì œì•½ì‚¬í•­**
   - âŒ **ë‹¨ì¼ ë¨¸ì‹ ì—ì„œë§Œ ì‘ë™** (shared memory ì œì•½)
   - âŒ **ë„¤íŠ¸ì›Œí¬ ë¶„ì‚° ë¶ˆê°€**
   - âŒ distributed-llamaëŠ” ë‹¤ì¤‘ ë¨¸ì‹  ì§€ì› í•„ìš”

---

## ğŸ”¨ ì‹œë„í•œ ì•Œê³ ë¦¬ì¦˜ë“¤

### ì‹œë„ 1: Binary Tree All-Gather (Recursive Doubling)

**ì´ë¡ ì  ë³µì¡ë„**: O(log n)

**ì•Œê³ ë¦¬ì¦˜:**
```
Step k: ê±°ë¦¬ 2^kì¸ ë…¸ë“œë¼ë¦¬ ë°ì´í„° êµí™˜
Step 0: 0â†”1, 2â†”3, 4â†”5, 6â†”7
Step 1: 0â†”2, 1â†”3, 4â†”6, 5â†”7
Step 2: 0â†”4, 1â†”5, 2â†”6, 3â†”7
â†’ logâ‚‚(n) ë‹¨ê³„ í›„ ì™„ë£Œ
```

**ì‹¤íŒ¨ ì›ì¸:**
```
Node 0: Level 2ì—ì„œ Node 4ë¡œë¶€í„° receive ëŒ€ê¸°
Node 4: Level 0ì—ì„œ Node 5ë¡œë¶€í„° receive ëŒ€ê¸°
â†’ ìˆœí™˜ ëŒ€ê¸° (Circular Wait) ë°œìƒ!
```

**ë¬¸ì œì :**
- âŒ ê° ë…¸ë“œê°€ ëª¨ë“  levelì„ **ìˆœì°¨ ì²˜ë¦¬**
- âŒ í•œ levelì—ì„œ blockë˜ë©´ ë‹¤ìŒ level ì§„í–‰ ë¶ˆê°€
- âŒ ë°ë“œë½ ë°œìƒ

---

### ì‹œë„ 2: Ring All-Gather

**ì´ë¡ ì  ë³µì¡ë„**: O(n)

**ì•Œê³ ë¦¬ì¦˜:**
```
Ring topology: 0 â†’ 1 â†’ 2 â†’ ... â†’ n-1 â†’ 0

ê° ë‹¨ê³„: ëª¨ë“  ë…¸ë“œê°€ ë™ì‹œì—
- ì˜¤ë¥¸ìª½ neighborì—ê²Œ send
- ì™¼ìª½ neighborë¡œë¶€í„° receive

n-1 ë‹¨ê³„ í›„ ëª¨ë“  ë…¸ë“œê°€ ì „ì²´ ë°ì´í„° ë³´ìœ 
```

**ì‹¤íŒ¨ ì›ì¸:**
```cpp
if (threadIndex != 0) return;  // Thread 1,2,3 ì¦‰ì‹œ ì¢…ë£Œ
// Thread 0ë§Œ í†µì‹ 
// â†’ Thread synchronization ê¹¨ì§
// â†’ ë‹¤ìŒ ì‘ì—…ì—ì„œ ith < nth assertion ì‹¤íŒ¨
```

**ë¬¸ì œì :**
- âŒ Thread ë™ê¸°í™” ì‹¤íŒ¨
- âŒ C++11ì—ëŠ” thread barrier ì—†ìŒ
- âŒ Implicit barrier ë¶ˆì¶©ë¶„

---

### ì‹œë„ 3: Binary Tree Gather-Broadcast

**ì´ë¡ ì  ë³µì¡ë„**: O(log n) + O(log n) = O(log n)

**ì•Œê³ ë¦¬ì¦˜:**
```
Phase 1: Binary Tree Gather (Bottom-Up)
  Level 0: 1â†’0, 3â†’2, 5â†’4, 7â†’6
  Level 1: 2â†’0, 6â†’4
  Level 2: 4â†’0
  â†’ Rootê°€ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘

Phase 2: Binary Tree Broadcast (Top-Down)
  Level 0: 0â†’1, 0â†’2, 0â†’4
  Level 1: 2â†’3, 4â†’5, 6â†’7
  â†’ ëª¨ë“  ë…¸ë“œê°€ ë°ì´í„° ìˆ˜ì‹ 
```

**ì‹¤íŒ¨ ì›ì¸:**
- ì‹œë„ 1ê³¼ ë™ì¼í•œ ìˆœí™˜ ëŒ€ê¸° ë¬¸ì œ
- Non-blocking socketì—ì„œ ë³µì¡í•œ ìˆœì„œ ì œì–´ ì‹¤íŒ¨

---

## âœ… ìµœì¢… í•´ê²°ì±…: Star Topology Gather-Broadcast

### ì•„í‚¤í…ì²˜

**Phase 1: Gather to Root (O(n))**
```
Worker 1 â”€â”€â”
Worker 2 â”€â”€â”¤
Worker 3 â”€â”€â”œâ”€â”€> ROOT (Node 0) [ëª¨ë“  slice ìˆ˜ì§‘]
Worker 4 â”€â”€â”¤
Worker 5 â”€â”€â”¤
Worker 6 â”€â”€â”¤
Worker 7 â”€â”€â”˜
```

**Phase 2: Broadcast from Root (O(n))**
```
Worker 1 <â”€â”€â”
Worker 2 <â”€â”€â”¤
Worker 3 <â”€â”€â”œâ”€â”€â”€ ROOT (Node 0) [ì™„ì „í•œ ë°ì´í„° ì „ì†¡]
Worker 4 <â”€â”€â”¤
Worker 5 <â”€â”€â”¤
Worker 6 <â”€â”€â”¤
Worker 7 <â”€â”€â”˜
```

### í•µì‹¬ ì½”ë“œ

```cpp
static void syncNodeSlices_starGatherBroadcast(
    bool onlyFromWorkerToRoot, NnNetwork *network, 
    NnUint nodeIndex, NnUint nNodes, NnByte *buffer, 
    NnSize nBytes, NnUint nThreads, NnUint threadIndex) {
    
    NnSize sliceBytes = nBytes / nNodes;
    
    // ========== PHASE 1: GATHER TO ROOT ==========
    if (nodeIndex == 0) {
        // ROOT: ë©€í‹°ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ìˆ˜ì‹ 
        NnUint nWorkers = nNodes - 1;
        NnUint workersPerThread = nWorkers / nThreads + 
                                  (nWorkers % nThreads > threadIndex ? 1 : 0);
        
        for (NnUint i = 0; i < workersPerThread; i++) {
            NnUint workerIdx = threadIndex + i * nThreads + 1;
            if (workerIdx < nNodes) {
                NnSocketIo io;
                io.socketIndex = workerIdx - 1;
                io.data = &buffer[sliceBytes * workerIdx];
                io.size = sliceBytes;
                network->readMany(1, &io);
            }
        }
    } else {
        // WORKER: Thread 0ë§Œ ì „ì†¡, ë‚˜ë¨¸ì§€ëŠ” ëŒ€ê¸° (synchronization)
        if (threadIndex == 0) {
            NnSocketIo io;
            io.socketIndex = 0;
            io.data = &buffer[sliceBytes * nodeIndex];
            io.size = sliceBytes;
            network->writeMany(1, &io);
        }
        // ëª¨ë“  threadê°€ í•¨ê»˜ ì¢…ë£Œ (implicit barrier)
    }
    
    if (onlyFromWorkerToRoot) return;
    
    // ========== PHASE 2: BROADCAST FROM ROOT ==========
    if (nodeIndex == 0) {
        // ROOT: ë©€í‹°ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì „ì†¡
        NnUint nWorkers = nNodes - 1;
        NnUint workersPerThread = nWorkers / nThreads + 
                                  (nWorkers % nThreads > threadIndex ? 1 : 0);
        
        for (NnUint i = 0; i < workersPerThread; i++) {
            NnUint workerIdx = threadIndex + i * nThreads + 1;
            if (workerIdx < nNodes) {
                NnSocketIo io;
                io.socketIndex = workerIdx - 1;
                io.data = buffer;
                io.size = nBytes;
                network->writeMany(1, &io);
            }
        }
    } else {
        // WORKER: Thread 0ë§Œ ìˆ˜ì‹ , ë‚˜ë¨¸ì§€ëŠ” ëŒ€ê¸°
        if (threadIndex == 0) {
            NnSocketIo io;
            io.socketIndex = 0;
            io.data = buffer;
            io.size = nBytes;
            network->readMany(1, &io);
        }
        // ëª¨ë“  threadê°€ í•¨ê»˜ ì¢…ë£Œ
    }
}
```

### í•µì‹¬ ì„¤ê³„ ì›ì¹™

1. **ë‹¨ìˆœì„± (Simplicity)**
   - Star topology: ë³µì¡í•œ peer-to-peer ì œê±°
   - ëª…í™•í•œ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤
   - Root ì¤‘ì‹¬ ì œì–´

2. **ë°ë“œë½ ë°©ì§€ (Deadlock-Free)**
   - ìˆœí™˜ ëŒ€ê¸° ë¶ˆê°€ëŠ¥ (Star topology)
   - ROOTê°€ ìˆœì°¨ ì²˜ë¦¬
   - ëª…í™•í•œ send/receive ìˆœì„œ

3. **Thread ë™ê¸°í™” (Thread Synchronization)**
   - ROOT: ëª¨ë“  threadê°€ ì‘ì—… ë¶„ë‹´
   - WORKER: Thread 0ë§Œ í†µì‹ , ë‚˜ë¨¸ì§€ëŠ” ì•”ë¬µì  ëŒ€ê¸°
   - ëª¨ë“  threadê°€ í•¨ê»˜ í•¨ìˆ˜ ì¢…ë£Œ (implicit barrier)

4. **ë³‘ë ¬ ì²˜ë¦¬ (Parallelism)**
   - ROOTì˜ 4ê°œ threadê°€ 7ëª… worker ë¶„ë‹´
   - Thread 0: Worker 1, 5
   - Thread 1: Worker 2, 6
   - Thread 2: Worker 3, 7
   - Thread 3: Worker 4

---

## ğŸ“Š ì„±ëŠ¥ ê²°ê³¼

### í…ŒìŠ¤íŠ¸ í™˜ê²½
- **ë…¸ë“œ ìˆ˜**: 8 nodes
- **ìŠ¤ë ˆë“œ ìˆ˜**: 4 threads per node
- **ëª¨ë¸**: LLaMA-3 8B (Q40 quantization)

### ì†Œí˜• ëª¨ë¸ ê²°ê³¼

| ë©”íŠ¸ë¦­ | ê¸°ì¡´ O(nÂ²) | ìƒˆ Star O(n) | ê°œì„ ìœ¨ |
|--------|-----------|--------------|--------|
| **SYNC_NODE_SLICES ì‘ì—… ìˆ˜** | 245 ops | 179 ops | **27% â†“** |
| **ì´ ë°ì´í„° ì „ì†¡** | 8.11 MB | 5.94 MB | **27% â†“** |
| **í‰ê·  ë ˆì´í„´ì‹œ** | 5.76 ms | 12.91 ms | - |
| **ì „ì²´ Throughput** | - | - | - |

### ëŒ€í˜• ëª¨ë¸ ê²°ê³¼

| ë©”íŠ¸ë¦­ | ê¸°ì¡´ O(nÂ²) | ìƒˆ Star O(n) | ê°œì„ ìœ¨ |
|--------|-----------|--------------|--------|
| **í‰ê·  ë ˆì´í„´ì‹œ** | 93.70 ms | 14.84 ms | **6.3ë°° â†“** |
| **Evaluation** | 0.56 tok/s | **1.52 tok/s** | **2.7ë°° â†‘** ğŸš€ |
| **Prediction** | 0.54 tok/s | **0.86 tok/s** | **1.6ë°° â†‘** ğŸš€ |

### í†µì‹  ë³µì¡ë„ ë¹„êµ

**8 ë…¸ë“œ ê¸°ì¤€:**
- **All-to-All O(nÂ²)**: 8 Ã— 7 = 56íšŒ í†µì‹ 
- **Star O(n)**: 7 (gather) + 7 (broadcast) = **14íšŒ í†µì‹ **
- **ê°œì„ **: 56 â†’ 14 = **4ë°° ê°ì†Œ!**

**16 ë…¸ë“œ ê¸°ì¤€:**
- **All-to-All O(nÂ²)**: 16 Ã— 15 = 240íšŒ í†µì‹ 
- **Star O(n)**: 15 + 15 = **30íšŒ í†µì‹ **
- **ê°œì„ **: 240 â†’ 30 = **8ë°° ê°ì†Œ!**

---

## ğŸ” ì™œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ì²˜ëŸ¼ ëª»í–ˆë‚˜?

### TensorRT-LLM / vLLM (GPU)

**ì‚¬ìš© ê¸°ìˆ :**
```
NCCL â†’ GPU ê¸°ë°˜ í†µì‹ 
â”œâ”€ CUDA streams (ë¹„ë™ê¸° ì²˜ë¦¬)
â”œâ”€ GPU memory bandwidth (ì´ˆê³ ì†)
â”œâ”€ Hardware optimized
â””â”€ Ring + Pipelining
```

**distributed-llamaì™€ì˜ ì°¨ì´:**
| íŠ¹ì„± | TensorRT/vLLM | distributed-llama |
|------|---------------|-------------------|
| **í•˜ë“œì›¨ì–´** | GPU (CUDA) | **CPU** |
| **í†µì‹ ** | NCCL | **TCP Sockets** |
| **ë™ê¸°í™”** | CUDA events | **C++ threads** |
| **ë³‘ë ¬ì„±** | ìˆ˜ì²œ CUDA threads | 4-8 threads |
| **ë©”ëª¨ë¦¬** | GPU global memory | **System RAM** |

### vLLM (CPU ëª¨ë“œ)

**ì‚¬ìš© ê¸°ìˆ :**
```
Shared Memory IPC (í”„ë¡œì„¸ìŠ¤ ê°„ ë©”ëª¨ë¦¬ ê³µìœ )
â”œâ”€ SHM segments (ê³µìœ  ë©”ëª¨ë¦¬)
â”œâ”€ wait_for_all() barrier
â”œâ”€ Zero network I/O
â””â”€ ë‹¨ì¼ ë¨¸ì‹  ì „ìš©
```

**distributed-llamaì™€ì˜ ì°¨ì´:**
| íŠ¹ì„± | vLLM CPU | distributed-llama |
|------|----------|-------------------|
| **ë°°í¬** | ë‹¨ì¼ ë¨¸ì‹  | **ë‹¤ì¤‘ ë¨¸ì‹ ** |
| **í†µì‹ ** | Shared memory | **Network** |
| **í”„ë¡œì„¸ìŠ¤** | ë©€í‹°í”„ë¡œì„¸ìŠ¤ | **ë©€í‹°ìŠ¤ë ˆë“œ** |
| **ë™ê¸°í™”** | SHM barriers | **ì—†ìŒ (C++11)** |

---

## âš ï¸ distributed-llamaì˜ ì œì•½ì‚¬í•­

### 1. C++11ì˜ í•œê³„

**ë¬¸ì œ:**
- C++11ì—ëŠ” `std::barrier` ì—†ìŒ (C++20ë¶€í„° ë„ì…)
- Thread ë™ê¸°í™” primitives ë¶€ì¡±

**ì˜í–¥:**
```cpp
// ë¶ˆê°€ëŠ¥í•œ ì½”ë“œ (C++20 ì´ìƒ)
std::barrier sync_point(nThreads);
if (threadIndex == 0) {
    // í†µì‹ 
}
sync_point.arrive_and_wait();  // ëª¨ë“  thread ëŒ€ê¸°
```

**ìš°ë¦¬ì˜ í•´ê²°ì±…:**
```cpp
// C++11ì—ì„œì˜ implicit barrier
if (threadIndex == 0) {
    // Thread 0ë§Œ í†µì‹ 
}
// ëª¨ë“  threadê°€ ì—¬ê¸°ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë‚¨
// í•¨ê»˜ return
```

### 2. ë„¤íŠ¸ì›Œí¬ í†µì‹ ì˜ ë³µì¡ì„±

**vLLM (shared memory):**
```
ë°ì´í„° ë³µì‚¬: 10-100 GB/s (ë©”ëª¨ë¦¬ ì†ë„)
ë™ê¸°í™”: lock-free, atomic operations
```

**distributed-llama (network):**
```
ë°ì´í„° ì „ì†¡: 1-10 Gbps (ë„¤íŠ¸ì›Œí¬ ì†ë„)
ë™ê¸°í™”: blocking sockets, ìˆœì„œ ì œì–´ í•„ìš”
```

### 3. Full Mesh Topology

**ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°:**
```
8 ë…¸ë“œ = 28ê°œ ì†Œì¼“ ì—°ê²° (n(n-1)/2)
ê° ë…¸ë“œê°€ ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œì™€ ì§ì ‘ ì—°ê²°
â†’ Socket ì¸ë±ìŠ¤ ë§¤í•‘ ë³µì¡
```

**ì†Œì¼“ ë§¤í•‘:**
```cpp
// Root (Node 0): socket[i] â†’ worker[i+1]
socket[0] â†’ Worker 1
socket[1] â†’ Worker 2
...

// Worker i: socket[0] â†’ root, socket[j] â†’ other workers
socket[0] â†’ Root
socket[1..n-2] â†’ Other workers (ìì‹  ì œì™¸)
```

---

## ğŸ’¡ ìµœì¢… í•´ê²° ë°©ë²•: Star Topologyì˜ ì¥ì 

### 1. ë‹¨ìˆœí•œ í†µì‹  íŒ¨í„´

**All-to-All (ë³µì¡):**
```
ê° ë…¸ë“œê°€ n-1ê°œ peerì™€ í†µì‹ 
Socket ë§¤í•‘ ë³µì¡
ìˆœì„œ ì œì–´ ì–´ë ¤ì›€
```

**Star (ë‹¨ìˆœ):**
```
Worker: Rootì™€ë§Œ í†µì‹  (1ê°œ ì—°ê²°)
ROOTë§Œ ë³µì¡í•œ ë©€í‹°ì†Œì¼“ ì²˜ë¦¬
ëª…í™•í•œ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤
```

### 2. Thread Synchronization

**ROOT (ë³‘ë ¬ ì²˜ë¦¬):**
```cpp
// 4ê°œ threadê°€ 7ëª… worker ë¶„ë‹´
Thread 0: Worker 1, 5 (socket 0, 4)
Thread 1: Worker 2, 6 (socket 1, 5)
Thread 2: Worker 3, 7 (socket 2, 6)
Thread 3: Worker 4    (socket 3)
â†’ ë³‘ë ¬ receive/send
```

**WORKER (ë™ê¸°í™” ë³´ì¥):**
```cpp
if (threadIndex == 0) {
    // í†µì‹ 
}
// Thread 1,2,3ì€ ì—¬ê¸°ì„œ ëŒ€ê¸°
// ëª¨ë“  threadê°€ í•¨ê»˜ ì¢…ë£Œ
â†’ Implicit barrier íš¨ê³¼
```

### 3. ë°ë“œë½ ì—†ìŒ

**ìˆœí™˜ ëŒ€ê¸° ë¶ˆê°€ëŠ¥:**
```
Star topologyì´ë¯€ë¡œ ëª¨ë“  í†µì‹ ì´ ROOT ê²½ìœ 
Worker â†” Worker ì§ì ‘ í†µì‹  ì—†ìŒ
â†’ ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ
â†’ ë°ë“œë½ ë¶ˆê°€ëŠ¥
```

---

## ğŸ¯ ì„±ëŠ¥ ë¶„ì„

### ì™œ ê°œë³„ ë ˆì´í„´ì‹œëŠ” ì¦ê°€í–ˆì§€ë§Œ throughputì€ í–¥ìƒë˜ì—ˆë‚˜?

**ê¸°ì¡´ O(nÂ²) All-to-All:**
```
ì¥ì : ëª¨ë“  threadê°€ ë³‘ë ¬ë¡œ ë‹¤ë¥¸ ë…¸ë“œì™€ í†µì‹ 
     â†’ ê°œë³„ operationì´ ë¹ ë¦„
ë‹¨ì : ë„ˆë¬´ ë§ì€ í†µì‹  (245 ops)
     â†’ ì „ì²´ì ìœ¼ë¡œ ëŠë¦¼
```

**ìƒˆ Star O(n):**
```
ì¥ì : í†µì‹  íšŸìˆ˜ ëŒ€í­ ê°ì†Œ (245 â†’ 179)
     â†’ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê°ì†Œ
     â†’ Throughput í–¥ìƒ (1.6-2.7ë°°!)
ë‹¨ì : ìˆœì°¨ ì²˜ë¦¬ë¡œ ê°œë³„ operationì€ ì¡°ê¸ˆ ëŠë¦´ ìˆ˜ ìˆìŒ
```

**ê²°ë¡ :** 
- ê°œë³„ latency < ì „ì²´ throughput
- **ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜ì€ throughputì´ ì¤‘ìš”** âœ…

### ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ í™œìš©

```
ê¸°ì¡´: 59.23 Mbps (ëŒ€í˜• ëª¨ë¸), 15.53 Mbps (ì†Œí˜• ëª¨ë¸)
ìƒˆ:   ë³‘ë ¬ ì²˜ë¦¬ë¡œ ROOTì˜ ëŒ€ì—­í­ íš¨ìœ¨ì  í™œìš©
```

---

## ğŸš€ í–¥í›„ ê°œì„  ë°©ì•ˆ

### Option 1: O(log n) Binary Tree ì¬ë„ì „

**ë°©ë²•:**
- ëª…ì‹œì  thread barrier êµ¬í˜„
- Atomic operations í™œìš©
- ê° level ê°„ ì „ì—­ ë™ê¸°í™”

**ì˜ˆìƒ ë³µì¡ë„:**
- O(log n) ë‹¬ì„± ê°€ëŠ¥
- í•˜ì§€ë§Œ êµ¬í˜„ ë³µì¡ë„ ë†’ìŒ

### Option 2: Pipelined Star

**ì•„ì´ë””ì–´:**
```
ë°ì´í„°ë¥¼ ì—¬ëŸ¬ chunkë¡œ ë¶„í• 
ROOTê°€ chunk ë‹¨ìœ„ë¡œ pipeline ì²˜ë¦¬
â†’ Latency hiding
```

**ì˜ˆìƒ íš¨ê³¼:**
- ê°œë³„ latency ê°ì†Œ
- Throughput ì¶”ê°€ í–¥ìƒ

### Option 3: Hybrid Approach

**ì „ëµ:**
```
if (nNodes <= 4):
    All-to-All (ì˜¤ë²„í—¤ë“œ ì‘ìŒ)
elif (nNodes <= 16):
    Star (O(n))
else:
    Binary Tree (O(log n))
```

### Option 4: NCCL/MPI Integration

**ì¥ê¸° ëª©í‘œ:**
- NCCL CPU backend í†µí•© (ê°€ëŠ¥í•˜ë‹¤ë©´)
- ë˜ëŠ” MPI library ì‚¬ìš©
- ê²€ì¦ëœ collective communication

---

## ğŸ“ˆ ë³µì¡ë„ ë¹„êµí‘œ

| ì•Œê³ ë¦¬ì¦˜ | ë³µì¡ë„ | 4 ë…¸ë“œ | 8 ë…¸ë“œ | 16 ë…¸ë“œ | 32 ë…¸ë“œ |
|---------|--------|--------|--------|---------|---------|
| **All-to-All** | O(nÂ²) | 12 | 56 | 240 | 992 |
| **Star** | **O(n)** | **6** | **14** | **30** | **62** |
| **Binary Tree** | O(log n) | 4 | 6 | 8 | 10 |
| **ê°œì„  (Allâ†’Star)** | - | 2ë°° | **4ë°°** | **8ë°°** | **16ë°°** |

---

## ğŸ› ï¸ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### Thread Distribution (ROOT)

```cpp
// 7ëª…ì˜ workerë¥¼ 4ê°œ threadì— ë¶„ì‚°
nWorkers = 7
nThreads = 4

workersPerThread = 7/4 + (7%4 > threadIndex ? 1 : 0)

Thread 0: 7/4 + (3 > 0 ? 1 : 0) = 1 + 1 = 2 workers (Worker 1, 5)
Thread 1: 7/4 + (3 > 1 ? 1 : 0) = 1 + 1 = 2 workers (Worker 2, 6)
Thread 2: 7/4 + (3 > 2 ? 1 : 0) = 1 + 1 = 2 workers (Worker 3, 7)
Thread 3: 7/4 + (3 > 3 ? 1 : 0) = 1 + 0 = 1 worker  (Worker 4)

ì´í•©: 2 + 2 + 2 + 1 = 7 âœ…
```

### Socket Index Mapping

```cpp
static inline NnUint getSocketIndexForNode(NnUint myNodeIndex, NnUint peerNode) {
    if (myNodeIndex == 0) {
        // Root: socket[i] â†’ worker[i+1]
        return peerNode - 1;
    }
    
    if (peerNode == 0) {
        // Worker to root: always socket[0]
        return 0;
    }
    
    // Worker to worker: skip self in socket array
    if (peerNode < myNodeIndex) {
        return peerNode;
    } else {
        return peerNode - 1;
    }
}
```

---

## ğŸ“š ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ ë¹„êµ ì •ë¦¬

### TensorRT-LLM

**ì¥ì :**
- âœ… NCCL â†’ ìµœê³  ì„±ëŠ¥ (GPU)
- âœ… O(log n) Ring + Pipelining
- âœ… ê²€ì¦ëœ NVIDIA ë¼ì´ë¸ŒëŸ¬ë¦¬

**ì œì•½:**
- âŒ GPU í•„ìˆ˜ (ë¹„ìš©)
- âŒ CUDA ì˜ì¡´ì„±

### vLLM (GPU)

**ì¥ì :**
- âœ… NCCL backend
- âœ… PyTorch í†µí•©
- âœ… ì‚¬ìš© í¸ì˜ì„±

**ì œì•½:**
- âŒ GPU í•„ìˆ˜

### vLLM (CPU)

**ì¥ì :**
- âœ… Shared Memory â†’ ë§¤ìš° ë¹ ë¦„
- âœ… Zero network I/O
- âœ… Thread-safe primitives

**ì œì•½:**
- âŒ **ë‹¨ì¼ ë¨¸ì‹ ë§Œ ì§€ì›**
- âŒ ë‹¤ì¤‘ ë¨¸ì‹  ë¶„ì‚° ë¶ˆê°€

### distributed-llama

**ì¥ì :**
- âœ… **ë‹¤ì¤‘ ë¨¸ì‹  ì§€ì›**
- âœ… **CPU ì „ìš©** (ì €ë¹„ìš©)
- âœ… Raw sockets (ì˜ì¡´ì„± ìµœì†Œ)

**ì œì•½:**
- âŒ NCCL ì‚¬ìš© ë¶ˆê°€
- âŒ Shared memory ì‚¬ìš© ë¶ˆê°€ (ë‹¤ì¤‘ ë¨¸ì‹ )
- âŒ C++11 (barrier ì—†ìŒ)

**í•´ê²°ì±…:**
- âœ… **Star Topology O(n)**
- âœ… Implicit thread synchronization
- âœ… 1.6-2.7ë°° throughput í–¥ìƒ

---

## ğŸ“ êµí›ˆ (Lessons Learned)

### 1. ì´ë¡  vs ì‹¤ì œ

**ì´ë¡ ì ìœ¼ë¡œ ìµœì„ :**
- Binary Tree: O(log n)

**ì‹¤ì œë¡œ êµ¬í˜„ ê°€ëŠ¥í•œ ìµœì„ :**
- Star Topology: O(n)
- ë‹¨ìˆœí•¨, ì•ˆì •ì„±, êµ¬í˜„ ê°€ëŠ¥ì„±

### 2. Thread Synchronizationì˜ ì¤‘ìš”ì„±

**ì‹¤íŒ¨í•œ ì ‘ê·¼:**
```cpp
if (threadIndex != 0) return;  // âŒ ë‹¤ë¥¸ thread ì¦‰ì‹œ ì¢…ë£Œ
```

**ì„±ê³µí•œ ì ‘ê·¼:**
```cpp
if (threadIndex == 0) {
    // í†µì‹ 
}
// ëª¨ë“  threadê°€ ì—¬ê¸° ë„ë‹¬ âœ…
```

### 3. í”„ë ˆì„ì›Œí¬ë³„ ìµœì  ê¸°ìˆ  ì„ íƒ

| í™˜ê²½ | ìµœì  ê¸°ìˆ  |
|------|----------|
| **GPU í´ëŸ¬ìŠ¤í„°** | NCCL + Ring |
| **ë‹¨ì¼ ë¨¸ì‹  CPU** | Shared Memory |
| **ë‹¤ì¤‘ ë¨¸ì‹  CPU** | **Star Topology** |

---

## ğŸ”§ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ (Star O(n) - ê¶Œì¥)

```bash
# ì´ë¯¸ í™œì„±í™”ë¨
make clean && make dllama
./dllama inference ...
```

### ê¸°ì¡´ All-to-Allë¡œ ë¡¤ë°±

`src/nn/nn-network.cpp` 1038-1042ì¤„:
```cpp
// 1039ì¤„ ì£¼ì„ ì²˜ë¦¬
// syncNodeSlices_starGatherBroadcast(...);

// 1042ì¤„ ì£¼ì„ í•´ì œ
syncNodeSlices_alltoall(...);
```

---

## ğŸ“– ì°¸ê³  ìë£Œ

### NCCL
- [NCCL User Guide](https://docs.nvidia.com/deeplearning/nccl/user-guide/)
- [NCCL Algorithms](https://github.com/NVIDIA/nccl/blob/master/doc/ALGORITHMS.md)

### vLLM
- [vLLM Distributed Communication](https://github.com/vllm-project/vllm/tree/main/vllm/distributed)
- CPU Shared Memory: `csrc/cpu/shm.cpp`

### TensorRT-LLM
- [AllGather Plugin](https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/plugins/ncclPlugin/allgatherPlugin.cpp)
- [Collective Operations](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/functional.py#L4142)

### MPI Algorithms
- MPI_Allgather: Ring and Recursive Doubling
- [Open MPI Implementation](https://github.com/open-mpi/ompi)

---

## âœ… ê²°ë¡ 

**ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±:**
- âœ… O(nÂ²) â†’ **O(n)** ìµœì í™”
- âœ… **1.6-2.7ë°° throughput í–¥ìƒ**
- âœ… ì•ˆì •ì  ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›
- âœ… ë‹¤ì¤‘ ë¨¸ì‹  ë¶„ì‚° í™˜ê²½ì—ì„œ ì‘ë™

**distributed-llamaì˜ ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ìµœì„ ì˜ í•´ê²°ì±…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!** ğŸ‰

í–¥í›„ O(log n)ì„ ì›í•œë‹¤ë©´:
1. C++20ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ (std::barrier ì‚¬ìš©)
2. ë˜ëŠ” MPI library í†µí•©
3. ë˜ëŠ” ëª…ì‹œì  barrier êµ¬í˜„

í˜„ì¬ Star O(n) êµ¬í˜„ìœ¼ë¡œë„ ì¶©ë¶„íˆ í›Œë¥­í•œ ì„±ëŠ¥ ê°œì„ ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤! ğŸš€


